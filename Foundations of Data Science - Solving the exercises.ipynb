{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Science - Solving the exercises\n",
    "\n",
    "This notebook is an experiment with two purposes:\n",
    "\n",
    "  * Analyze if jupyter + GitHub is a viable blogging platform.\n",
    "  * Learn more about data science.\n",
    "  \n",
    "My idea is to do detailed solutions of each exercise, using the embedded Python to create plots and do numerical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "The first two items ask to compute the expected values $E(x)$, $E(x^2)$, $E(x - y)$, $E(xy)$ and $E(x - y)^2$ for two cases:\n",
    "\n",
    "  * When $x$ and $y$ are uniform variables in the interval $[0, 1]$.\n",
    "  * When $x$ and $y$ are uniform variables in the interval $[-\\frac{1}{2}, \\frac{1}{2}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item 2.1.1\n",
    "\n",
    "By symmetry we know that\n",
    "\n",
    "$E(x) = \\frac{1}{2}$ and\n",
    "\n",
    "$E(x - y) = 0$.\n",
    "\n",
    "Computing directly the other expressions:\n",
    "\n",
    "$E(x^2) = \\int_0^1 dx\\,x^2 = \\left.\\frac{x^3}{3}\\right|_0^1 = \\frac{1}{3}$\n",
    "\n",
    "$E(xy) = \\int_0^1 dx \\int_0^1 dy\\,xy = \\int_0^1 dx\\,x \\int_0^1 dy\\,y = \\left(\\int_0^1 dx\\,x\\right)^2 = \\left(\\left. \\frac{x^2}{2}\\right|_0^1\\right)^2 = \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4}$\n",
    "\n",
    "$E(x - y)^2 = E(x^2 - 2xy + y^2) = E(x^2) - 2E(xy) + E(y^2) = \\frac{1}{3} - 2\\cdot\\frac{1}{4} + \\frac{1}{3} = \\frac{1}{6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numpy to quickly check the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from scipy import stats\n",
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = stats.uniform.rvs(loc=0.0, scale=1.0, size=10000)\n",
    "y = stats.uniform.rvs(loc=0.0, scale=1.0, size=10000)\n",
    "print 'E(x) ~=', np.average(x)\n",
    "print 'E(x^2) ~=', np.average(x ** 2)\n",
    "print 'E(x - y) ~=', np.average(x - y)\n",
    "print 'E(xy) ~=', np.average(x * y)\n",
    "print 'E(x - y)^2 ~=', np.average((x - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Item 2.1.2\n",
    "\n",
    "Now by symmetry we know that\n",
    "\n",
    "$E(x) = 0$,\n",
    "\n",
    "$E(x - y) = 0$ and\n",
    "\n",
    "$E(xy) = 0$.\n",
    "\n",
    "By translational invariance,\n",
    "\n",
    "$E(x - y)^2 = \\frac{1}{6}$ like in the previous item.\n",
    "\n",
    "We can compute the variance directly,\n",
    "\n",
    "$E(x^2) = \\int_{-\\frac{1}{2}}^{\\frac{1}{2}} dx\\,x^2 = 2\\int_0^{\\frac{1}{2}} dx\\,x^2 = 2\\left.\\frac{x^3}{3}\\right|_0^{\\frac{1}{2}} = 2\\frac{\\frac{1}{8}}{3} = \\frac{1}{12}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = stats.uniform.rvs(loc=-0.5, scale=1.0, size=10000)\n",
    "y = stats.uniform.rvs(loc=-0.5, scale=1.0, size=10000)\n",
    "print 'E(x) ~=', np.average(x)\n",
    "print 'E(x^2) ~=', np.average(x ** 2)\n",
    "print 'E(x - y) ~=', np.average(x - y)\n",
    "print 'E(xy) ~=', np.average(x * y)\n",
    "print 'E(x - y)^2 ~=', np.average((x - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item 2.1.3\n",
    "\n",
    "As every coordinate of each point will be independent, the result will be given by multiplying the value of $E(x - y)^2$ by $d$,\n",
    "\n",
    "$E(\\mathrm{dist}) = \\frac{d}{6}$.\n",
    "\n",
    "Checking for $d = 24$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def point_rv(d):\n",
    "    return stats.uniform.rvs(loc=-0.5, scale=1.0, size=d)\n",
    "def dist_rv(d): \n",
    "    return np.sum((point_rv(d) - point_rv(d)) ** 2)\n",
    "print 'E(dist) ~=', np.average([dist_rv(24) for _ in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "We start by getting the points,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = np.vstack(point_rv(100) for _ in range(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then use the pairwise distance function from SciPy to calculate the distances and angles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "distances = pdist(points, 'euclidean')\n",
    "angles = np.arccos(1 - pdist(points, 'cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have to plot the resulting values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plot(distances, angles, '.'); xlabel('distances'); ylabel('angles');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it is clear that the vectors are almost orthogonal (angles around $\\pi/2$) and with distances clustered around a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "We can trivially solve both items by using a nonnegative \"random variable\" $x$ such that $P(x = 0) = 1$. Then, of course, $E(x) = 0$ and $P(x \\ge a) = E(x) / a = 0$ for all positive $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "If we take a random variable $x$ such that $P(x = -1) = P(x = +1) = \\frac{1}{2}$, then we clearly have $Var(x) = E(x^2) - E(x)^2 = 1 - 0 = 1$ and, if we choose $c = 1$, we get\n",
    "\n",
    "$P(|x - E(x)| \\ge 1) = P(|x| \\ge 1) = 1 = \\frac{1}{1^2} = \\frac{Var(x)}{c^2}$.\n",
    "\n",
    "To get a non-tight bound, it's as easy as using $c = 2$:\n",
    "\n",
    "$P(|x - E(x)| \\ge 1) = P(|x| \\ge 1) = 0 \\lt \\frac{1}{2^2} = \\frac{Var(x)}{c^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5\n",
    "\n",
    "#### Item 2.5.1\n",
    "\n",
    "We just have to impose normalization:\n",
    "\n",
    "$\\int_{-\\infty}^\\infty dx\\,p(x) = 1$\n",
    "\n",
    "$\\int_1^\\infty dx \\frac{c}{x^4} = 1$\n",
    "\n",
    "$\\left. -\\frac{c}{4 x^3} \\right|_1^\\infty = 1$\n",
    "\n",
    "$\\frac{c}{1^3} = 4$\n",
    "\n",
    "$c = 4$\n",
    "\n",
    "#### Item 2.5.2 \n",
    "\n",
    "Calculating the expected value:\n",
    "\n",
    "$\\int_{-\\infty}^\\infty dx\\,x p(x) = \\int_1^\\infty \\frac{dx\\,4}{x^3} = \\left. -\\frac{4}{3 x^2} \\right|_1^\\infty = \\frac{4}{3}$\n",
    "\n",
    "Simulating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'E(x) ~=', np.average(stats.pareto.rvs(b=3, size=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 100 samples we expect a fractional deviation around 0.1, so the result is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "\n",
    "We can just apply the definitions:\n",
    "\n",
    "$E(\\|\\mathbf{x}\\|^2) = E(\\sum_{i=1}^d x_i^2) = \\sum_{i=1}^d E(x_i^2) = \\sum_{i=1}^d \\frac{1}{2} = \\frac{d}{2}$\n",
    "\n",
    "Checking for $d = 50$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sph_normal_point_rv(d, sigma):\n",
    "    return stats.norm.rvs(loc=0.0, scale=np.sqrt(sigma), size=d)\n",
    "points = np.vstack(sph_normal_point_rv(50, 0.5) for _ in range(1000))\n",
    "sq_dists = np.sum(points ** 2, axis=-1)\n",
    "print 'E(||x||^2) ~=', np.average(sq_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it clearly matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7\n",
    "\n",
    "By cheating (reading [this post](http://stats.stackexchange.com/a/85962)), we get the following simple argument. Let's assume that our point in the surface of the sphere is $\\mathbf{x}$. If we now start from the fact that the norm of $\\mathbf{x}$ is 1,\n",
    "\n",
    "$E(\\|\\mathbf{x}\\|^2) = 1$,\n",
    "\n",
    "then we can replace and expand by linearity of expectation:\n",
    "\n",
    "$E(\\sum_{i=1}^d x_i^2) = 1$\n",
    "\n",
    "$\\sum_{i=1}^d E(x_i^2) = 1$.\n",
    "\n",
    "Using the fact that all coordinates share the same distribution,\n",
    "\n",
    "$E(x_i^2) = \\frac{1}{d}$,\n",
    "\n",
    "we get that the variance of any coordinate is $\\frac{1}{d}$ in $\\mathbb{R}^d$.\n",
    "\n",
    "Checking by simulation for $d = 7$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sn_points = np.vstack(sph_normal_point_rv(7, 1.0) for _ in range(10000))\n",
    "sph_surf_points = sn_points / np.sqrt(np.sum(sn_points ** 2, axis=-1)).reshape(-1, 1)\n",
    "print 'E(x1^2) ~=', np.average(sph_surf_points[:,0] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see it matches accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8\n",
    "\n",
    "This can be solved by an elementary scaling argument. If the volume of the $d$-dimensional unit ball is $V(d)$, then we want $\\epsilon$ such that\n",
    "\n",
    "$V(d)(1 - \\epsilon)^d = \\frac{V(d)}{100}$.\n",
    "\n",
    "Doing some elementary algebra, we get\n",
    "\n",
    "$(1 - \\epsilon)^d = \\frac{1}{100}$\n",
    "\n",
    "$\\log\\,(1 - \\epsilon)^d = \\log \\frac{1}{100}$\n",
    "\n",
    "$d \\log (1 - \\epsilon) = -\\log 100$\n",
    "\n",
    "$1 - \\epsilon = e^{-\\frac{\\log 100}{d}}$\n",
    "\n",
    "$\\epsilon = 1 - e^{-\\frac{\\log 100}{d}}$,\n",
    "\n",
    "where $\\log$ is the natural logarithm.\n",
    "\n",
    "If we simulate points using rejection sampling (for simplicity/reliability) and putting it together with the bands in a single plot, we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ball_point_rv(d):\n",
    "    p = np.ones(d)\n",
    "    while np.sum(p ** 2) >= 1.0:\n",
    "        p = stats.uniform.rvs(loc=-1.0, scale=2.0, size=d)\n",
    "    return p\n",
    "point_norms = np.array([np.sqrt(np.sum(ball_point_rv(d) ** 2)) for _ in range(100) for d in range(1, 11)])\n",
    "point_xs = np.array([stats.uniform.rvs(loc=d, scale=1.0) for _ in range(100) for d in range(1, 11)])\n",
    "%matplotlib notebook\n",
    "plot(point_xs, point_norms, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2.9\n",
    "\n",
    "#### Item 2.9.1\n",
    "\n",
    "We just have to see how the coordinates of a $k$-dimensional face of a $d$-dimensional cube look like. WLOG we can assume the cube is $[0, 1]^d$. Then the 0-dimensional ones (vertices) are just vectors of ones and zeros, a total of $2^d$ different ones. A 1-dimensional face (edge) joins two vertices that differ in a single coordinate, so we can see each edge as a vector of $d - 1$ definite 0-1 values and one free parameter $x$, so we are going to have $d 2^{d - 1}$ different ones. In general then we will have $\\binom{d}{k}2^{d - k}$ $k$-dimensional faces in a $d$-dimensional cube.\n",
    "\n",
    "Doing some sanity check for $d = 2$ and $d = 3$:\n",
    "\n",
    "$\\binom{2}{0}2^2 = 4$ (vertices in a square)\n",
    "\n",
    "$\\binom{2}{1}2^1 = 4$ (edges in a square)\n",
    "\n",
    "$\\binom{2}{2}2^0 = 1$ (squares in a square)\n",
    "\n",
    "$\\binom{3}{0}2^3 = 8$ (vertices in a cube)\n",
    "\n",
    "$\\binom{3}{1}2^2 = 12$ (edges in a cube)\n",
    "\n",
    "$\\binom{3}{2}2^1 = 6$ (faces in cube)\n",
    "\n",
    "$\\binom{3}{3}2^0 = 1$ (cubes in a cube)\n",
    "\n",
    "So it seems to work well.\n",
    "\n",
    "#### Item 2.9.2\n",
    "\n",
    "This is just a question of summing,\n",
    "\n",
    "$\\displaystyle \\sum_{k=0}^d \\binom{d}{k} 2^{d-k}$,\n",
    "\n",
    "but it's not easy to intuit the result of that sum. Checking the numbers for different values of $d$:\n",
    "\n",
    "$d = 1 \\rightarrow \\binom{1}{0}2^1 + \\binom{1}{1}2^0 = 2 + 1 = 3$\n",
    "\n",
    "$d = 2 \\rightarrow \\binom{2}{0}2^2 + \\binom{2}{1}2^1 + \\binom{2}{2}2^0 = 4 + 4 + 1 = 9$\n",
    "\n",
    "$d = 3 \\rightarrow \\binom{3}{0}2^3 + \\binom{3}{1}2^2 + \\binom{3}{2}2^1 + \\binom{3}{3}2^0 = 8 + 12 + 6 + 1 = 27$\n",
    "\n",
    "The result clearly seems to go like powers of three... and in fact it's not very surprising, as every face can be described as a vector of 1s, 0s and free parameters, that could all be represented with a single simbol, $x$ for example. But using induction to get a formal relationship,\n",
    "\n",
    "**Base:** \n",
    "\n",
    "$\\displaystyle \\sum_{k=0}^0 \\binom{0}{k} 2^{0 - k} = \\binom{0}{0} 2^0 = 1 = 3^0$\n",
    "\n",
    "**Inductive:**\n",
    "\n",
    "$\\displaystyle \\sum_{k=0}^{d + 1} \\binom{d + 1}{k} 2^{(d + 1) - k} = \\sum_{k=0}^{d + 1} \\left(\\binom{d}{k - 1} + \\binom{d}{k}\\right) 2^{(d + 1) - k}$ (Pascal's rule)\n",
    "\n",
    "$\\displaystyle = \\sum_{k=0}^{d + 1} \\binom{d}{k - 1} 2^{(d + 1) - k} + \\sum_{k=0}^{d + 1} \\binom{d}{k} 2^{(d + 1) - k}$ (distributing)\n",
    "\n",
    "$\\displaystyle = \\sum_{m = -1}^d \\binom{d}{m} 2^{(d + 1) - (m + 1)} + 2 \\sum_{k=0}^{d + 1} \\binom{d}{k} 2^{d - k}$ (changing index, extracting 2)\n",
    "\n",
    "$\\displaystyle = \\sum_{m = 0}^d \\binom{d}{m} 2^{d - m} + 2 \\sum_{k=0}^d \\binom{d}{k} 2^{d - k}$ (removing null terms)\n",
    "\n",
    "$\\displaystyle = 3 \\sum_{k=0}^d \\binom{d}{k} 2^{d - k}$\n",
    "\n",
    "$\\displaystyle = 3 \\cdot 3^d$\n",
    "\n",
    "$\\displaystyle = 3^{d + 1}$\n",
    "\n",
    "#### Item 2.9.3\n",
    "\n",
    "As every face has area 1, the total area will be\n",
    "\n",
    "$\\displaystyle A = \\binom{d}{d - 1}2^{d - (d - 1)} = 2 d$.\n",
    "\n",
    "#### Item 2.9.4\n",
    "\n",
    "It would be scaled by $2^{d - 1}$, so it would be $d\\,2^d$.\n",
    "\n",
    "#### Item 2.9.5\n",
    "\n",
    "As the \"erosion\" of a cube by a sphere is a smaller cube, we can see how deep we have to go to get 90% of the volume.\n",
    "\n",
    "$(1 - 2h)^d = 0.1$\n",
    "\n",
    "$\\log (1 - 2h)^d = \\log 0.1$\n",
    "\n",
    "$d \\log (1 - 2h) = \\log 0.1$\n",
    "\n",
    "$\\log (1 - 2h) = \\frac{1}{d}\\log 0.1$\n",
    "\n",
    "$1 - 2h = \\exp\\left(\\frac{1}{d}\\log 0.1\\right)$\n",
    "\n",
    "$h = \\frac{1}{2}\\left(1 - \\exp\\left(\\frac{1}{d}\\log 0.1\\right)\\right)$\n",
    "\n",
    "It's clear that $h \\rightarrow 0$ as $d \\rightarrow \\infty$. And, in more concrete terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "d = 100\n",
    "print 'h =', 0.5 * (1 - math.exp(1.0 / d * math.log(0.1))), 'for d =', d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning that 90% of the volume of the unit cube is within a surface layer of thickness 0.012 for $d = 100$.\n",
    "\n",
    "### Exercise 2.10\n",
    "\n",
    "The area is a ring of thickness $d\\theta$ and length $2\\pi\\sin\\theta$, so with area $d\\theta\\,2\\pi\\sin\\theta$.\n",
    "\n",
    "Integrating we get,\n",
    "\n",
    "$\\displaystyle A(\\Theta) = \\int_0^\\Theta d\\theta\\,2\\pi\\sin\\theta$\n",
    "\n",
    "$\\displaystyle = -2\\pi \\left.\\cos\\theta\\right|_0^\\Theta$\n",
    "\n",
    "$\\displaystyle = -2\\pi \\left(\\cos\\Theta - \\cos 0\\right)$\n",
    "\n",
    "$\\displaystyle = 2\\pi \\left(1 - \\cos\\Theta\\right)$,\n",
    "\n",
    "and it matches the sanity check for $\\Theta = \\pi/2$ and $\\Theta = \\pi$:\n",
    "\n",
    "$\\displaystyle A(\\pi/2) = 2\\pi\\left(1 - \\cos\\frac{pi}{2}\\right) = 2\\pi(1 - 0) = 2\\pi$\n",
    "\n",
    "$\\displaystyle A(\\pi) = 2\\pi\\left(1 - \\cos\\pi\\right) = 2\\pi(1 - (-1)) = 4\\pi$.\n",
    "\n",
    "In the especial case where the angle is $36Â°$, we can just put $\\pi/5$:\n",
    "\n",
    "$\\displaystyle A(\\pi) = 2\\pi\\left(1 - \\cos\\frac{\\pi}{5}\\right) = 2\\pi\\left(1 - \\frac{\\sqrt{5} + 1}{4}\\right) = \\frac{3 - \\sqrt{5}}{2}\\pi$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
